{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Keras NLP - BERT Base Multi Sentiment Analysis example for cross-validation\n",
        "\n",
        "An example usage of sentiment analysis with a BERT Base Multi model using Keras NLP / Keras Hub. This notebook is for Google Colab, use at least the T4 GPU. This notebook was used for cross-validation, where the cross-validation folds have been made already created and saved. Then they are loaded one by one (not optimal, it would be better to preprocess the data and then create the folds)."
      ],
      "metadata": {
        "id": "I3NAG84bIPEs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First install keras_nlp (or keras_hub which is the same thing right now)"
      ],
      "metadata": {
        "id": "YZqmXPx2_s3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_nlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgV0LPTnjdlU",
        "outputId": "bb3fbc20-6e3e-4f75-c696-2de7cb39b8e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras_nlp in /usr/local/lib/python3.10/dist-packages (0.17.0)\n",
            "Requirement already satisfied: keras-hub==0.17.0 in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (0.17.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras-hub==0.17.0->keras_nlp) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras-hub==0.17.0->keras_nlp) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-hub==0.17.0->keras_nlp) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from keras-hub==0.17.0->keras_nlp) (2024.9.11)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras-hub==0.17.0->keras_nlp) (13.9.4)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.10/dist-packages (from keras-hub==0.17.0->keras_nlp) (0.3.4)\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.10/dist-packages (from keras-hub==0.17.0->keras_nlp) (2.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kagglehub->keras-hub==0.17.0->keras_nlp) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kagglehub->keras-hub==0.17.0->keras_nlp) (4.66.6)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-hub==0.17.0->keras_nlp) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-hub==0.17.0->keras_nlp) (2.18.0)\n",
            "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-hub==0.17.0->keras_nlp) (4.12.2)\n",
            "Requirement already satisfied: tensorflow<2.19,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text->keras-hub==0.17.0->keras_nlp) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras-hub==0.17.0->keras_nlp) (0.1.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (2.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (1.67.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (3.5.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (0.37.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras-hub==0.17.0->keras_nlp) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras-hub==0.17.0->keras_nlp) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras-hub==0.17.0->keras_nlp) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras-hub==0.17.0->keras_nlp) (2024.8.30)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (0.45.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (0.13.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Imports"
      ],
      "metadata": {
        "id": "EnZnMJhNAW_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras import layers\n",
        "import keras_nlp\n",
        "import keras_hub"
      ],
      "metadata": {
        "id": "G_w-pgAxhWuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setup\n",
        "\n",
        "KERAS_BACKEND specifies which backend is used for computation. Can choose from tensorflow, pytorch and jax. Second line specifies precision policy."
      ],
      "metadata": {
        "id": "rE5v-31tAuCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "keras.mixed_precision.set_global_policy(\"mixed_float16\")"
      ],
      "metadata": {
        "id": "QO_gISakJPYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading data\n",
        "\n",
        "First, unzip the data (or load them in a different way). File available at https://github.com/immm00/diplomka/blob/main/datasets/splits/extracted/cross_val_folds_extracted.zip."
      ],
      "metadata": {
        "id": "3ldZPugrByhN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip cross_val_folds_extracted.zip"
      ],
      "metadata": {
        "id": "qOeDt6lKhy7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data is loaded using text_dataset_from_directory, which expects a specific directory structure. Folders are separated into train and test. Furthermore, each folder contains subfolders for classes - in this case positive, negative and neutral. Inside then are individual text files. Each text file is one instance (line of text).\n",
        "\n",
        "Validation data is not used since this notebook was used for cross-validation.\n",
        "\n",
        "Data will be processed in batches. Batch size is set to 32 here."
      ],
      "metadata": {
        "id": "OyBrr1H4ukPN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wxoW6wzhSrO",
        "outputId": "350fe0ff-535d-4cdf-a72c-ed8f497cdc02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6750 files belonging to 3 classes.\n",
            "Found 750 files belonging to 3 classes.\n",
            "Number of batches in raw_train_ds: 211\n",
            "Number of batches in raw_test_ds: 24\n"
          ]
        }
      ],
      "source": [
        "\n",
        "batch_size = 32\n",
        "raw_train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"cross_val_folds/fold_1/train\",\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "raw_test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"cross_val_folds/fold_1/test\", batch_size=batch_size\n",
        ")\n",
        "\n",
        "print(f\"Number of batches in raw_train_ds: {raw_train_ds.cardinality()}\")\n",
        "print(f\"Number of batches in raw_test_ds: {raw_test_ds.cardinality()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initializing the model\n",
        "\n",
        "Using keras_nlp, a specific pretrained model is loaded as a part of a classifier. Available pretrained models are listed here: https://keras.io/keras_hub/presets/.\n",
        "\n",
        "Bert_base_multi, a multilingual model is used, as there are no pretrained models for Czech specifically available. It is pretrained on wikipedias of different languages.\n",
        "\n",
        "The number of classes is set to 3 (positive, negative, neutral).\n",
        "\n",
        "The summary shows the layers, parameters, etc. Part of the classifier is a preprocessor (BertTokenizer). Tokenization will happen automatically, there is no need to preprocess the data beforehand.\n",
        "\n",
        "On the extracted economics dataset, the 3 epochs will take around 10 minutes with the T4 GPU."
      ],
      "metadata": {
        "id": "0BZZp3O5DBrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = keras_nlp.models.BertClassifier.from_preset(\n",
        "    \"bert_base_multi\",\n",
        "    num_classes=3,\n",
        ")\n",
        "\n",
        "#classifier.summary()\n"
      ],
      "metadata": {
        "id": "b39yiQygjDNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fine-tuning\n",
        "\n",
        "The pretrained model needs to be fine-tuned for the sentiment analysis task. The training and validation data is used for this. The number of epochs refers to how many times a machine learning model goes through the entire training dataset during training. It is set to 3 here.\n",
        "\n",
        "The output will show step number, time, and evaluation metrics (loss function and sparse categorical accuracy). Since there is no validation data, it only shows metrics for training data."
      ],
      "metadata": {
        "id": "QvUoKuRvEVUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.fit(\n",
        "    raw_train_ds,\n",
        "    epochs=3\n",
        ")"
      ],
      "metadata": {
        "id": "z0FmCWsto17w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c61da46b-4482-473c-c628-d0a381ad36c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m  3/211\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:05\u001b[0m 892ms/step - loss: 1.1235 - sparse_categorical_accuracy: 0.2899"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluation\n",
        "\n",
        "Since the evaluation metrics provided by the evaluate function are limited, it is necessary to calculate them manually. Prediction is done for the training data and then metrics like recall, precision and f-score is calculated using the real and predicted labels."
      ],
      "metadata": {
        "id": "mGckkYdCLtSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "predictions = []\n",
        "true_labels = []\n",
        "texts_list = []\n",
        "\n",
        "for texts, labels in raw_test_ds:\n",
        "    preds = classifier.predict(texts)\n",
        "    preds = np.argmax(preds, axis=-1)\n",
        "    predictions.extend(preds)\n",
        "    true_labels.extend(labels.numpy())\n",
        "\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "f1 = f1_score(true_labels, predictions, average='macro')\n",
        "\n",
        "precision_total = precision_score(true_labels, predictions, average='weighted')\n",
        "recall_total = recall_score(true_labels, predictions, average='weighted')\n",
        "\n",
        "precision_per_class = precision_score(true_labels, predictions, average=None)\n",
        "recall_per_class = recall_score(true_labels, predictions, average=None)\n",
        "f1_per_class = f1_score(true_labels, predictions, average=None)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Macro F1 Score: {f1:.4f}\")\n",
        "print(f\"Total Precision (Weighted): {precision_total:.4f}\")\n",
        "print(f\"Total Recall (Weighted): {recall_total:.4f}\")\n",
        "\n",
        "for i, (prec, rec, f1_val) in enumerate(zip(precision_per_class, recall_per_class, f1_per_class)):\n",
        "    print(f\"Class {i}: Precision = {prec:.4f}, Recall = {rec:.4f}, F1 Score = {f1_val:.4f}\")"
      ],
      "metadata": {
        "id": "Tzso9sVij-K1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}