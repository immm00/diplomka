{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6940bec8-f236-48f6-8962-57462bd20694",
   "metadata": {},
   "source": [
    "# Rule-based Sentiment Analysis for Czech\n",
    "\n",
    "All code for rule-based sentiment analysis. It contains functions for loading datasets and lexicons, preprocessing data, the implementation of rule-based algorithms, and evaluation.\n",
    "\n",
    "Most of the code blocks are just definitions of functions. They are then used at the end to carry out the sentiment analysis in the evaluation part.\n",
    "\n",
    "To see the evaluations and examples, run all blocks before the Evaluation section. Then choose a block after that to run. As preprocessing whole datasets takes some time, the preprocessing has already been done and preprocessed datasets are serialized using the Pickle library.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7129c1-b388-4a76-8d85-3b3430b9de3a",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Libraries used are pandas, sklearn, spacy, spacy_udpipe, pickle, and a few others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d73a5d6-b60f-4506-b35f-21ad458fd4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already downloaded a model for the 'cs' language\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold\n",
    "import spacy_udpipe\n",
    "spacy_udpipe.download('cs')\n",
    "#nlp = spacy_udpipe.load('cs')\n",
    "from spacy import displacy\n",
    "from spacy.tokens import Doc, Token, Span\n",
    "from spacy.lang.cs import stop_words as stop_words\n",
    "from string import punctuation\n",
    "from collections import defaultdict\n",
    "from decimal import Decimal\n",
    "import pickle\n",
    "import re\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438c7cff-8ae0-4116-9595-45bbbd630c9d",
   "metadata": {},
   "source": [
    "## Functions for loading datasets\n",
    "\n",
    "Contains functions for loading datasets in various formats (xlsx, csv, txt, ...). They have parameters so that changes to the actual source file shouldn't be needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46070c5d-27e1-44b1-b53d-dc883db4224b",
   "metadata": {},
   "source": [
    "### Loads from two txt files - one containing texts, one labels - needs files names, label names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab278334-8a5a-4055-be4a-fc710ff28783",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loads from two txt files - one containing texts, one labels\n",
    "def load_from_txt(text_file, label_file, pos_label, neg_label, neu_label, encoding='utf-8'):\n",
    "    with open(text_file, 'r', encoding=encoding) as texts_file:\n",
    "        texts = texts_file.readlines()\n",
    "\n",
    "    with open(label_file, 'r', encoding=encoding) as labels_file:\n",
    "        labels = labels_file.readlines()\n",
    "\n",
    "    texts = [text.strip() for text in texts]\n",
    "    labels = [label.strip() for label in labels]\n",
    "    \n",
    "    data = {\n",
    "        'texts': texts,\n",
    "        'labels': labels\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    label_mapping = {pos_label: 'p', neg_label: 'n', neu_label: '0'}\n",
    "    df['labels'] = df['labels'].replace(label_mapping)\n",
    "    df = df[df['labels'].isin([pos_label, neg_label, neu_label])]\n",
    "\n",
    "    print('data loaded')\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bd0603-f8bd-4524-b204-838244bc88de",
   "metadata": {},
   "source": [
    "### Loads from txt files that seperately have positive, negative and neutral texts - needs files names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a64cefa3-88fa-4bd1-9c23-47faa0021bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loads from txt files that seperately habe positive, negative and neutral texts\n",
    "def load_from_separate_txts(pos_file, neg_file, neu_file, encoding='utf-8'):\n",
    "    with open(pos_file, 'r', encoding=encoding) as file:\n",
    "        pos_texts = file.readlines()\n",
    "        \n",
    "    with open(neg_file, 'r', encoding=encoding) as file:\n",
    "        neg_texts = file.readlines()\n",
    "        \n",
    "    with open(neu_file, 'r', encoding=encoding) as file:\n",
    "        neu_texts = file.readlines()\n",
    "\n",
    "    texts=[text.strip() for text in pos_texts]+[text.strip() for text in neg_texts]+[text.strip() for text in neu_texts]\n",
    "    labels=['p']*len(pos_texts)+['n']*len(neg_texts)+['0']*len(neu_texts)\n",
    "\n",
    "    data = {\n",
    "        'texts': texts,\n",
    "        'labels': labels\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    print('data loaded')\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd51076e-5ed7-431e-bd35-1cc246b82acb",
   "metadata": {},
   "source": [
    "### Loads from one csv file containg both texts and labels - needs file name, header names (text, label), label names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fd0746b-51a0-4d54-8b23-6659041b9dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loads from one csv file containg both texts and labels\n",
    "def load_from_csv(file_name, text_header, label_header, pos_label, neg_label, neu_label, encoding='utf-8', separator=','):\n",
    "    df = pd.read_csv(file_name, sep=separator, encoding=encoding, names=[text_header, label_header])\n",
    "    df.rename(columns={text_header: 'texts', label_header: 'labels'}, inplace=True)\n",
    "\n",
    "    label_mapping = {pos_label: 'p', neg_label: 'n', neu_label: '0'}\n",
    "    df['labels'] = df['labels'].replace(label_mapping)\n",
    "    df = df[df['labels'].isin([pos_label, neg_label, neu_label])]\n",
    "    \n",
    "    print('data loaded')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c8bd73-82cf-46b7-b92f-200f3c1189b5",
   "metadata": {},
   "source": [
    "### Loads from excel - needs file name, header names (text, label), label names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f5c1043-3fb9-48bc-8217-56a402db476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load from excel\n",
    "def load_from_excel(file_name, text_header, label_header, pos_label, neg_label, neu_label, sheet_name=0):\n",
    "    df = pd.read_excel(file_name, sheet_name=sheet_name, usecols=[text_header, label_header])\n",
    "    df.rename(columns={text_header: 'texts', label_header: 'labels'}, inplace=True)\n",
    "\n",
    "    label_mapping = {pos_label: 'p', neg_label: 'n', neu_label: '0'}\n",
    "    df['labels'] = df['labels'].replace(label_mapping)\n",
    "    df = df[df['labels'].isin([pos_label, neg_label, neu_label])]\n",
    "    \n",
    "    print('data loaded')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476bf533-1e4f-4742-95f4-a1a3c6aa9d77",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Preprocessing using the udpipe model, spacy_udpipe and spacy libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff3c89e-0467-4f6c-acd1-7aa72980be33",
   "metadata": {},
   "source": [
    "### Loading the udpipe model for preprocessing and setting up Spacy extensions\n",
    "\n",
    "The model for preprocessing is loaded using spacy_udpipe. The model comes from https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-3131."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c2c4ec6-ebbc-4561-9af6-5c9e845f2636",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy_udpipe.load_from_path(lang=\"cs\", path=\"./czech-pdt-ud-2.5-191206.udpipe\", meta={\"description\": \"cz model cac\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc7f0e5-3250-43da-93df-16c5993b24d0",
   "metadata": {},
   "source": [
    "### Returns lemmatized word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "999477d5-f595-4c30-9580-ea282c27b02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_word(word):\n",
    "    return nlp(word)[0].lemma_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c4e684-520c-4c94-a14e-85d9faea3ba5",
   "metadata": {},
   "source": [
    "### Some helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f8280a5-ac13-4db1-825f-82245f917243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(doc):\n",
    "    return [token for token in doc]\n",
    "\n",
    "#removes repeating punctuation\n",
    "def clean_text(text):\n",
    "    pattern = r'([^\\w\\s])\\1+'\n",
    "\n",
    "    cleaned_text = re.sub(pattern, r'\\1', text)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026f704d-87eb-4ca8-8c29-a6969c900ca4",
   "metadata": {},
   "source": [
    "### Preprocesses one text, return doc object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "046a99f0-70a3-41ba-b58b-a00b6592fc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    return nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92d5e17-cf7c-4f61-9c64-af25e3b5e498",
   "metadata": {},
   "source": [
    "### Preprocesses a whole dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd6498ce-be9c-4460-9c4d-d8d89f14c12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_texts(df):\n",
    "    print('starting preprocessing')\n",
    "\n",
    "    df['texts'] = df['texts'].apply(clean_text)\n",
    "\n",
    "    df['docs'] = list(nlp.pipe(df['texts']))\n",
    "    df['tokens'] = df['docs'].apply(get_tokens)\n",
    "    \n",
    "    print('finished preprocessing')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf70aad-33e8-4420-8ccf-eaeb630afc1d",
   "metadata": {},
   "source": [
    "## Serializing preprocessed data\n",
    "\n",
    "As preprocessing the data takes some time, it is useful to do it just once and then serialize it and load when it is needed. This is done using the Pickle library. Preprocessing CSFD takes a long time (took around 45 minutes), so it is disabled by default. To enable it, change cell type from raw to code. Preprocessing the other datasets shouldn't take too long. The serialized files are used during evaluation, so this part needs to be run before that. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7208352e-6ae6-4d6a-9dcd-f6def20be280",
   "metadata": {},
   "source": [
    "### Preprocess and serialize whole csfd dataset (runs for a long time - enable first)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "39cdf68e-53e8-48e7-9b61-1862bcf22179",
   "metadata": {},
   "source": [
    "def preprocess_serialize_csfd():\n",
    "    df = load_from_separate_txts('datasets/csfd/positive.txt','datasets/csfd/negative.txt','datasets/csfd/neutral.txt')\n",
    "    preprocessed_df = preprocess_texts(df)\n",
    "    preprocessed_df = preprocessed_df.drop('tokens', axis=1)\n",
    "    with open('datasets/csfd.pickle', 'wb') as f:\n",
    "        pickle.dump(preprocessed_df, f)\n",
    "\n",
    "preprocess_serialize_csfd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5645b08f-e998-47bc-a934-ad1d5ab3e944",
   "metadata": {},
   "source": [
    "### Preprocess and serialize whole facebook dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3585c965-4188-4c4b-a415-4c909982b400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n",
      "starting preprocessing\n",
      "finished preprocessing\n"
     ]
    }
   ],
   "source": [
    "def preprocess_serialize_facebook():\n",
    "    df = load_from_txt('datasets/facebook/gold-posts.txt','datasets/facebook/gold-labels.txt','p','n','0')\n",
    "    preprocessed_df = preprocess_texts(df)\n",
    "    preprocessed_df = preprocessed_df.drop('tokens', axis=1)\n",
    "    with open('datasets/facebook.pickle', 'wb') as f:\n",
    "        pickle.dump(preprocessed_df, f)\n",
    "\n",
    "preprocess_serialize_facebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c42b3d-8add-43e2-b084-7f92bb650aaa",
   "metadata": {},
   "source": [
    "### Preprocess and serialize whole synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0e686ea-9f1d-423f-9744-cd1089f13adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n",
      "starting preprocessing\n",
      "finished preprocessing\n"
     ]
    }
   ],
   "source": [
    "def preprocess_serialize_synthetic():\n",
    "    df = load_from_separate_txts('datasets/synthetic/positive.txt','datasets/synthetic/negative.txt','datasets/synthetic/neutral.txt')\n",
    "    preprocessed_df = preprocess_texts(df)\n",
    "    preprocessed_df = preprocessed_df.drop('tokens', axis=1)\n",
    "    with open('datasets/synthetic.pickle', 'wb') as f:\n",
    "        pickle.dump(preprocessed_df, f)\n",
    "\n",
    "preprocess_serialize_synthetic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a0d37a-5fd0-4521-a855-f4cd9ff8df64",
   "metadata": {},
   "source": [
    "### Preprocess and serialize whole extracted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d8112c3-87e7-4e65-975b-3aa6ccee8034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n",
      "starting preprocessing\n",
      "finished preprocessing\n"
     ]
    }
   ],
   "source": [
    "def preprocess_serialize_extracted():\n",
    "    extracted_dataset = load_from_excel('datasets/extracted/extracted_dataset.xlsx', 'Text', 'Immer', 'p', 'n', '0')\n",
    "    preprocessed_df = preprocess_texts(extracted_dataset)\n",
    "    preprocessed_df = preprocessed_df.drop('tokens', axis=1)\n",
    "    with open('datasets/extracted.pickle', 'wb') as f:\n",
    "        pickle.dump(preprocessed_df, f)\n",
    "\n",
    "preprocess_serialize_extracted()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da41be39-0a9b-49c7-b467-f294ead533c2",
   "metadata": {},
   "source": [
    "### Load from pickle, restore tokens column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "336eb1b6-83e9-4cf9-803c-8a5f58e2c0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_pickle(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        df = pickle.load(file)\n",
    "    df['tokens'] = df['docs'].apply(get_tokens)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11e3029-e4ce-43ec-9f22-80f5a386c1e0",
   "metadata": {},
   "source": [
    "## Loading and generating lexicons\n",
    "\n",
    "Contains functions for loading lexicons (Sublex - https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-FF60-B, Affin.CZ - https://github.com/VilemR/affin.cz) and for automatic generation of lexicon from existing data. The loaded lexicons can be either a dataframe or a dictionary, with the dictionary being the final product that is then used later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d698ac3-137b-49dd-b490-2ce79dc2e51b",
   "metadata": {},
   "source": [
    "### Loads lexicon from csv (as a dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f77d851-4e08-4cf5-b783-8c8f819a06c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lexicon_from_csv(file_name, lemma_header='lemma', label_header='sentiment_value', encoding='utf-8', separator=','):\n",
    "    df = pd.read_csv(file_name, sep=separator, encoding=encoding)\n",
    "\n",
    "    df.rename(columns={lemma_header: 'lemma', label_header: 'sentiment_value'}, inplace=True)\n",
    "    \n",
    "    print('lexicon loaded from '+file_name)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed0fe49-e5bd-4f60-99c6-413b68ca25d7",
   "metadata": {},
   "source": [
    "### Loads czech sublex (as a dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79de9c58-933c-4107-bd4e-540fa545e3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_after_underscore(s):\n",
    "    return s.split('_')[0]\n",
    "\n",
    "def load_czech_sublex(file_name):\n",
    "    column_names = ['negation', 'pos', 'lemma', 'sentiment_value', 'src']\n",
    "\n",
    "    df = pd.read_csv(file_name, sep='\\t', header=None, names=column_names, engine ='python')\n",
    "    \n",
    "    replacement_map = {'NEG': -1.0, 'POS': 1.0}\n",
    "    df['sentiment_value'] = df['sentiment_value'].map(replacement_map).astype(float)\n",
    "\n",
    "    df['lemma'] = df['lemma'].apply(remove_after_underscore)\n",
    "    \n",
    "    print('lexicon loaded from '+file_name)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3a7999-98e6-45f3-816f-c45eb3904f27",
   "metadata": {},
   "source": [
    "### Loads czech sublex (as a dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2856209-9499-4a08-ae4b-f87a7ef35be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_czech_sublex_dict(file_name):    \n",
    "    return df_lexicon_to_dict(load_czech_sublex(file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839989ef-a1db-42c5-bc67-7df22a90b390",
   "metadata": {},
   "source": [
    "### Loads AFFIN CZ lexicon\n",
    "As the Affin.CZ lexicon isn't fully lemmatized, lemmatization is applied here during loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23a4025d-0b93-4441-896b-faa757364195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_affincz(file_name):\n",
    "    affin = load_lexicon_from_csv(file_name, lemma_header='word_cz', label_header='polarity', separator=',')\n",
    "    \n",
    "    lemmatized_dict = {}\n",
    "\n",
    "    def process_row(row):\n",
    "        word = row['lemma']\n",
    "        label = row['sentiment_value']\n",
    "        lemma = lemmatize_word(word)\n",
    "        if word == 'není-li': # lematized as 'být' which would be incorrect\n",
    "            lemmatized_dict['není-li']=-1\n",
    "            return\n",
    "        if word.startswith('ne'):\n",
    "            if not lemma.startswith('ne') and lemma[0] == word[2]:\n",
    "                lemmatized_dict['ne'+lemma] = label\n",
    "            else:\n",
    "                lemmatized_dict[lemma] = label\n",
    "        else:\n",
    "            lemmatized_dict[lemma] = label  \n",
    "\n",
    "    # Apply the function to each row in the DataFrame\n",
    "    affin.apply(process_row, axis=1)\n",
    "\n",
    "    return lemmatized_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0408961b-761c-4561-a48d-d00b179cd892",
   "metadata": {},
   "source": [
    "### Loads vulgarisms lexicon as dict (all negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0112d5d2-57db-4061-970e-d6813b00be49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vulgarisms(file_name):\n",
    "    vulgarisms_dict = {}\n",
    "    \n",
    "    with open(file_name, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        vulgarisms_dict[line] = -1\n",
    "\n",
    "    print('lexicon loaded from '+file_name)\n",
    "    \n",
    "    return vulgarisms_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee5ad5f-d5bc-4395-9c4f-bcd6105a6fe3",
   "metadata": {},
   "source": [
    "### Converts dataframe lexicon into dict lexicon (columns 'lemma' and 'sentiment_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db6f88e3-5fba-403f-9994-19b576bb42f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_lexicon_to_dict(df):\n",
    "    return dict(zip(df['lemma'], df['sentiment_value']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b9a0b7-9da0-4ffd-82d9-7920708041f0",
   "metadata": {},
   "source": [
    "### Automatic lexicon generation\n",
    "\n",
    "Generates the lexicon - needs df with tokens (output of the preprocessing function) and labels. Parameter min_occurence is the minimal occurrence of a word in the data for it  to be used in the lexicon. Min_abs_value is the minimal absolute sentiment value a word must have to be in the lexicon. If simple_sentiment is True, it will round the sentiment values to either -1 or 1. Use additional_stats=True for counts and ratios used for calculating sentiment value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2902a03-6212-4b81-9ab4-5b325abb8af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lexicon(df, min_occurence=1, min_abs_value = 0.3, simple_sentiment=False, stop_words=stop_words.STOP_WORDS, additional_stats=False):\n",
    "    print('starting lexicon generation')\n",
    "    \n",
    "    lemma_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        label = row['labels']\n",
    "        tokens = row['tokens']\n",
    "        for token in tokens:\n",
    "            if token.lemma_ not in stop_words and token.pos_ in ['NOUN', 'VERB', 'ADJ', 'ADV']:\n",
    "                lemma_counts[label][token.lemma_] += 1\n",
    "\n",
    "    lemma_counts_df = pd.DataFrame(lemma_counts).fillna(0).astype(int)\n",
    "\n",
    "    lemma_counts_df['total'] = lemma_counts_df.sum(axis=1)\n",
    "\n",
    "    for label in df['labels'].unique():\n",
    "        lemma_counts_df[f'{label}_ratio'] = lemma_counts_df[label] / lemma_counts_df['total']\n",
    "\n",
    "    lemma_counts_df = lemma_counts_df[lemma_counts_df['total'] >= min_occurence]\n",
    "\n",
    "    lemma_counts_df['sentiment_value'] = -1 * lemma_counts_df.get('n_ratio', 0) + lemma_counts_df.get('p_ratio', 0)\n",
    "    lemma_counts_df = lemma_counts_df[lemma_counts_df['sentiment_value'] != 0]\n",
    "\n",
    "    lemma_counts_df = lemma_counts_df[abs(lemma_counts_df['sentiment_value']) >= min_abs_value]\n",
    "    if simple_sentiment:\n",
    "        lemma_counts_df['sentiment_value'] = lemma_counts_df['sentiment_value'].apply(lambda x: 1 if x > 0 else -1)\n",
    "        \n",
    "\n",
    "    lemma_counts_df.reset_index(inplace=True)\n",
    "    lemma_counts_df.rename(columns={'index': 'lemma'}, inplace=True)\n",
    "    \n",
    "    print('finished lexicon generation')\n",
    "    \n",
    "    if additional_stats:\n",
    "        return lemma_counts_df\n",
    "    else:\n",
    "        return lemma_counts_df[['lemma', 'sentiment_value']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0609ece7-52f3-418f-8a11-b014f7e60580",
   "metadata": {},
   "source": [
    "### Automatic lexicon generation - dict form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d74f7c2-ca83-4b21-ae94-da07dd96e698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lexicon_dict(df, min_occurence=1, min_abs_value=0.3, simple_sentiment=False, stop_words=stop_words.STOP_WORDS):\n",
    "    return df_lexicon_to_dict(generate_lexicon(df, min_occurence, min_abs_value, simple_sentiment, stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f84802-8ae9-4b12-8fbd-841fa4929c22",
   "metadata": {},
   "source": [
    "## Sentiment analysis implementation\n",
    "Besides loading and initializing things, there are two main parts - 1) looking up the sentiment value of a word in a lexicon, and 2) obtaining the sentiment of a whole text. Two approaches have been implemented for 2) - proximity approach and dependency tree approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84fb46a-99d3-4372-9c18-72e0cadc47b4",
   "metadata": {},
   "source": [
    "### Adding spacy extensions\n",
    "Spacy extensions are set so necessary information (sentiment, shifter type, etc.) can be added to Tokens and Spans. These extensions are then used by the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14524ba7-c733-456c-98b3-2bb90c9b9b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "Token.set_extension(\"sentiment\", default=0, force=True)\n",
    "Token.set_extension(\"shifter_type\", default='none', force=True)\n",
    "Token.set_extension(\"passed_shifters\", default=[], force=True)\n",
    "Token.set_extension(\"passed_sentiment\", default=0, force=True)\n",
    "Token.set_extension(\"ignore_as_shifter\", default=False, force=True)\n",
    "Span.set_extension(\"sentiment\", default=0, force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c13679-4724-4794-a92e-ab8361949a3e",
   "metadata": {},
   "source": [
    "### Loads shifter lists\n",
    "Returns negators, intensificators, deintensificators, adversatives (all in lemmatized form)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2f6b821-7cd2-4981-8a2c-da1e70752d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_shifters():\n",
    "    negators = []\n",
    "    intensificators = []\n",
    "    deintensificators = []\n",
    "    adversatives = []\n",
    "    \n",
    "    with open('text_resources/shifters/negators_cs.txt', 'r', encoding='utf-8') as file:\n",
    "        negators = [lemmatize_word(word) for word in file.readlines()]\n",
    "    \n",
    "    with open('text_resources/shifters/intensificators_cs.txt', 'r', encoding='utf-8') as file:\n",
    "        intensificators = [lemmatize_word(word) for word in file.readlines()]\n",
    "    \n",
    "    with open('text_resources/shifters/deintensificators_cs.txt', 'r', encoding='utf-8') as file:\n",
    "        deintensificators = [lemmatize_word(word) for word in file.readlines()]\n",
    "    \n",
    "    with open('text_resources/shifters/adversatives_cs.txt', 'r', encoding='utf-8') as file:\n",
    "        adversatives = [lemmatize_word(word) for word in file.readlines()]\n",
    "\n",
    "    return negators, intensificators, deintensificators, adversatives\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9114922f-0c8d-4a9c-86a1-1a0ea9195382",
   "metadata": {},
   "source": [
    "### Initializing some constants, lists, sets, helper methods\n",
    "Several thing are initialized here that are used by the algorithms later. This includes constants for shifter patterns, punctuation list, shifter lists, definition for the economics pattern and lists of emoticons. Also some helper methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "919dd746-21e7-46d6-8f70-dcaa6df47ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "constants = {\n",
    "    'negation_positive': -1,\n",
    "    'negation_negative': -0.5,\n",
    "    'intensification': 1.8,\n",
    "    'deintensification': 0.2,\n",
    "    'adversative_before': 1.5,\n",
    "    'adversative_after': 0.5\n",
    "}\n",
    "\n",
    "punct_list = ['.',',','?','!',';']\n",
    "\n",
    "negators, intensificators, deintensificators, adversatives = load_shifters()\n",
    "\n",
    "bigger_words = {'zvýšit', 'vysoký', 'vyšší', 'růst','roustoucí', 'sílící', 'zvyšující', 'roste', 'vzrůst', 'zvýšení', 'vzestup', 'vzestoupit', 'vzestupující','zvětšit', 'zvětšující', 'zesílit', 'zesílení', 'zesilující', 'posílit', 'posílení', 'posilující', 'velký','stoupnout','stoupnutí', 'stoupající'}\n",
    "smaller_words = {'snížit', 'snižující', 'nízký', 'nižší', 'pokles', 'poklesnout', 'snížení', 'klesnout', 'klesající', 'klesání','klesat', 'sestoupit', 'sestup', 'sestupující', 'menší', 'malý','klesnutí'}\n",
    "\n",
    "bigger_is_good_words = {'mzda','plat','ekonomika','HDP', 'hodnota', 'úrověň', 'zisk', 'výdělek', 'profit', 'tržba', 'zaměstnanost'}\n",
    "bigger_is_bad_words = {'cena','dluh','zadlužení','inflace', 'nezaměstnanost', 'schodek', 'deficit'}\n",
    "\n",
    "happy_emoticons  = set()\n",
    "with open('text_resources/other/happy_emoticons.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        happy_emoticons.add(line.strip())\n",
    "        \n",
    "sad_emoticons  = set()\n",
    "with open('text_resources/other/happy_emoticons.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        happy_emoticons.add(line.strip())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ece31f37-2966-4003-ba07-653df8048cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_token_by_lemma(tokens, lemma):\n",
    "    for token in tokens:\n",
    "        if token.lemma_ == lemma:\n",
    "            return token\n",
    "\n",
    "def get_nbor(token, pos):\n",
    "    try:\n",
    "        return token.nbor(pos).text\n",
    "    except (TypeError, IndexError) as e:\n",
    "         return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93fad55-0f90-4288-8778-a49b2e9ccf47",
   "metadata": {},
   "source": [
    "### Looking up sentiment of a word in a lexicon\n",
    "Returns sentiment of a given token using dict lookup, handles negation. Has fix for emoticons (the udpipe model used doesn't tokenize emoticons properly). See comments accompanying code for more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5ac8029-9033-47b2-92d6-043f6f96c1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_sentiment(token, lexicon, enable_negated_lemma=True):\n",
    "    #lexical negation (word starting with 'ne' - eg.'nehezký', 'nebezpečí' etc.) must be accounted for, as lemma from lexicon and lemmatizator aren't always the same\n",
    "    #eg. according to the cs udpipe model lemma for 'nebezpečí' is 'bezpečí', but the czech sublex only lists 'nebezpečí' as a polarized word\n",
    "    #if enable_negated_lemma is True, function will check for the negated version of the word and return a flipped sentiment value of that word\n",
    "    #eg. if lemma is 'nehezký' but lexicon only contains 'hezký' as positive, it will return negative - might not be 100 % accurate\n",
    "    \n",
    "    if token.text.startswith('ne'):\n",
    "            #both token and lemma start with 'ne' -> return sentiment if lemma in lexicon\n",
    "            if token.lemma_.startswith('ne') and token.lemma_ in lexicon:\n",
    "                return  lexicon[token.lemma_]    \n",
    "            if token.lemma_.startswith('ne') and token.lemma_ not in lexicon:\n",
    "                return 0\n",
    "\n",
    "            #token starts with 'ne' but lemma doesn't -> either it was lexical negation which got removed by lemmatization'ne' OR it wasn't a lexical negation (eg. 'nejoblíbenější' -> 'oblíbený')\n",
    "            #if the first char of lemma matches third char of original, we assume it was lexical negation (only 'ne' was removed) -> then we figure out which version (original or non-negated) is in lexicon\n",
    "            if not token.lemma_.startswith('ne') and len(token.text) > 3 and token.lemma_[0] == token.text[2]:\n",
    "                #neither are in lexicon -> return 0\n",
    "                if 'ne'+token.lemma_ not in lexicon and token.lemma_ not in lexicon:\n",
    "                    return 0   \n",
    "                #original (negated) is in lexicon -> return sentiment for original \n",
    "                if 'ne'+token.lemma_ in lexicon:\n",
    "                    return lexicon['ne'+token.lemma_]\n",
    "                #for negated polarized verbs\n",
    "                if token.lemma_ in lexicon and token.pos_=='VERB' and  'Neg' in token.morph.get('Polarity'):\n",
    "                    return -1 * lexicon[token.lemma_]\n",
    "                if token.lemma_ in lexicon:\n",
    "                    return -1 * lexicon[token.lemma_]\n",
    "            #it isn't lexical negation, but something like 'nejoblíbenější' -> 'oblíbený'\n",
    "            if token.lemma_ in lexicon:\n",
    "                return lexicon[token.lemma_]\n",
    "            return 0\n",
    "    \n",
    "    #token doesn't start with 'ne'\n",
    "    else:        \n",
    "        #lemma is in lexicon -> return sentiment value for lemma\n",
    "        if token.lemma_ in lexicon:\n",
    "            return lexicon[token.lemma_]\n",
    "        #(optionally) if lemma not in lexicon but 'ne' + lemma is (potentially negated version of that word), return flipped version of that \n",
    "        if enable_negated_lemma and 'ne'+token.lemma_ in lexicon:\n",
    "            return -1 * lexicon['ne'+token.lemma_]\n",
    "\n",
    "        #text emoticon fix\n",
    "        \n",
    "        if token.text in  [':',';'] and False:\n",
    "            nbor_minus_1 = get_nbor(token, -1)\n",
    "            nbor_minus_2 = get_nbor(token, -2)\n",
    "            nbor_plus_1 = get_nbor(token, 1)\n",
    "            nbor_plus_2 = get_nbor(token, 2)\n",
    "\n",
    "            possible_emoticons = [nbor_minus_1+nbor_minus_2+token.text,nbor_minus_1+token.text,token.text+nbor_plus_1,token.text+nbor_plus_1+nbor_plus_2]\n",
    "            if set(possible_emoticons).intersection(happy_emoticons):\n",
    "                return 1\n",
    "            if set(possible_emoticons).intersection(sad_emoticons):\n",
    "                return -1\n",
    "        \n",
    "        #lemma not in lexicon -> return 0\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f401823-5ae6-49f7-aa02-e15285331dc3",
   "metadata": {},
   "source": [
    "### Marks token as shifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f27e7b17-ac62-4822-be47-45492e7badfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_as_shifter(token):\n",
    "    if token.lemma_ in ['nikoliv', 'nikoli', 'ne', 'nikterak', 'nijak'] or (token.pos_ in ['VERB','AUX'] and\n",
    "                'Neg' in token.morph.get('Polarity')):\n",
    "        token._.shifter_type = 'neg'\n",
    "    elif token.lemma_ in intensificators:\n",
    "        token._.shifter_type = 'int'\n",
    "    elif token.lemma_ in deintensificators:\n",
    "        token._.shifter_type = 'deint'\n",
    "    elif token.lemma_ in adversatives:\n",
    "        token._.shifter_type = 'adv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232817fa-6ceb-47b2-ad61-e148b12a6696",
   "metadata": {},
   "source": [
    "### Applies shifter pattern to polarized token\n",
    "Depending on the shifter type, it applies the shifter pattern to the polarized token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a5ac192-84b3-4de6-ae66-5a0fb6e24957",
   "metadata": {},
   "outputs": [],
   "source": [
    "constants = {\n",
    "    'negation_positive': -1,\n",
    "    'negation_negative': -0.5,\n",
    "    'intensification': 1.8,\n",
    "    'deintensification': 0.2,\n",
    "    'adversative_before': 1.5,\n",
    "    'adversative_after': 0.5\n",
    "}\n",
    "\n",
    "shifter_counts = {\n",
    "    'neg': 0,\n",
    "    'int': 0,\n",
    "    'deint': 0,\n",
    "    'adv_before':0,\n",
    "    'adv_after':0\n",
    "}\n",
    "\n",
    "def apply_shifter(token, shifter_type, constants=constants, on_passed=False):\n",
    "    shifter_counts[shifter_type] = shifter_counts[shifter_type] + 1\n",
    "    if on_passed:\n",
    "        #print('applying shifter: ', token, shifter_type)\n",
    "        if shifter_type == 'neg':\n",
    "            if token._.passed_sentiment > 0:\n",
    "                token._.passed_sentiment = constants['negation_positive'] * token._.passed_sentiment\n",
    "            else:\n",
    "                token._.passed_sentiment = constants['negation_negative'] * token._.passed_sentiment          \n",
    "        if shifter_type == 'int':\n",
    "            token._.passed_sentiment = constants['intensification'] * token._.passed_sentiment \n",
    "        if shifter_type == 'deint':\n",
    "            token._.passed_sentiment = constants['deintensification'] * token._.passed_sentiment\n",
    "        if shifter_type == 'adv_before':\n",
    "            token._.passed_sentiment = constants['adversative_before'] * token._.passed_sentiment\n",
    "        if shifter_type == 'adv_after':\n",
    "            token._.passed_sentiment = constants['adversative_after'] * token._.passed_sentiment\n",
    "    else:\n",
    "        #print('applying shifter: ', token, shifter_type)\n",
    "        if shifter_type == 'neg':\n",
    "            if token._.sentiment > 0:\n",
    "                token._.sentiment = constants['negation_positive'] * token._.sentiment\n",
    "            else:\n",
    "                token._.sentiment = constants['negation_negative'] * token._.sentiment   \n",
    "        if shifter_type == 'int':\n",
    "            token._.sentiment = constants['intensification'] * token._.sentiment \n",
    "        if shifter_type == 'deint':\n",
    "            token._.sentiment = constants['deintensification'] * token._.sentiment\n",
    "        if shifter_type == 'adv_before':\n",
    "            token._.sentiment = constants['adversative_before'] * token._.sentiment\n",
    "        if shifter_type == 'adv_after':\n",
    "            token._.sentiment = constants['adversative_after'] * token._.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e20013a-aadc-4fd9-b5dd-2e1164979922",
   "metadata": {},
   "source": [
    "### Proximity-based shifter pattern approach\n",
    "Returns overall sentiment from a list of tokens using proximity-based shifter pattern approach. First, it marks shifter words and assigns sentiment to tokens using the lexicon lookup. Then, it applies the shifter patterns using proximity clusters around polarized words. Lastly, it iterates over the set of sentences to calculate overall sentiment. The economics pattern (bigger is better / bigger is worse) is mixed in with the usual shifters (negation, intensification, deintensification, adversatives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a91cfd1-669c-428e-bccd-87f401bf0aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_proxi(tokens, lexicon, before=4, after=2, constants = constants, debug=False):\n",
    "    \n",
    "    sentences = set()\n",
    "    \n",
    "    for token in tokens:\n",
    "\n",
    "        #some extensions have to be reset\n",
    "        token._.sentiment = 0\n",
    "        token._.ignore_as_shifter = False\n",
    "        \n",
    "        sentences.add(token.sent)\n",
    "\n",
    "        \n",
    "        mark_as_shifter(token)\n",
    "        \n",
    "        token._.sentiment = get_token_sentiment(token, lexicon)\n",
    "        \n",
    "    \n",
    "    for i in range(len(tokens)): # for every token\n",
    "        token = tokens[i]\n",
    "        if token._.sentiment != 0 or token.lemma_ in bigger_is_good_words.union(bigger_is_bad_words):\n",
    "\n",
    "            #creates a list of tokens preceeding the polarized token, based on the 'before' argument. Cuts it short if there is any punctuation\n",
    "            start_index = max(0, i-before)\n",
    "            pre_tokens = tokens[start_index:i]\n",
    "            for j in range(len(pre_tokens)):  \n",
    "                if pre_tokens[j].text in punct_list:\n",
    "                    start_index = j\n",
    "            pre_tokens = pre_tokens[start_index:]       \n",
    "                \n",
    "\n",
    "            #creates a list of tokens following the polarized token, based on the 'after' argument. Cuts it short if there is punctuation, if it's a comma, it looks one token further for adversative\n",
    "            end_index = min(len(tokens), i + after + 1)\n",
    "            foll_tokens = tokens[i + 1:end_index]\n",
    "            for j in range (len(foll_tokens)):\n",
    "                if foll_tokens[j] in punct_list:\n",
    "                    end_index = j\n",
    "                if foll_tokens[j] == ',' and foll_tokens[j].lemma_ in adversatives:\n",
    "                    end_index = min(len(tokens),j+1)       \n",
    "            foll_tokens = foll_tokens[:end_index]\n",
    "\n",
    "            context_tokens = pre_tokens + foll_tokens\n",
    "            \n",
    "            negators_count =  0\n",
    "\n",
    "            #assign polarity to economic term based on increase/decrease ('high inflation' -> negative)\n",
    "            #needs to happen before shifters are apllied\n",
    "            #if it was used in this way, it needs to be ignored as a shifter later\n",
    "            context_tokens_set = {t.lemma_ for t in context_tokens}\n",
    "            #increase of economic domain term            \n",
    "            intersection = context_tokens_set.intersection(bigger_words)\n",
    "            if intersection:\n",
    "                if token.lemma_ in bigger_is_good_words:\n",
    "                    token._.sentiment = 1\n",
    "                if token.lemma_ in bigger_is_bad_words:\n",
    "                    token._.sentiment = -1\n",
    "                used_lemma = list(intersection)[0]\n",
    "                ignored_token = find_token_by_lemma(context_tokens, used_lemma)\n",
    "                ignored_token._.ignore_as_shifter = True\n",
    "                \n",
    "            #decrease of economic domain term\n",
    "            intersection = context_tokens_set.intersection(smaller_words)\n",
    "            if intersection:\n",
    "                if token.lemma_ in bigger_is_good_words:\n",
    "                    token._.sentiment = -1\n",
    "                if token.lemma_ in bigger_is_bad_words:\n",
    "                    token._.sentiment = 1\n",
    "                used_lemma = list(intersection)[0]\n",
    "                ignored_token = find_token_by_lemma(context_tokens, used_lemma)\n",
    "                ignored_token._.ignore_as_shifter = True\n",
    "\n",
    "            debug and print('Evaluating context cluster of token ',token)\n",
    "            for context_token in context_tokens: \n",
    "                \n",
    "\n",
    "                if not context_token._.ignore_as_shifter:\n",
    "                \n",
    "                    #negation\n",
    "                    #Only first negator is counted, as additional negators won't have additional effect for usual polarized context ('nebylo to nikterak dobré' stays negative)\n",
    "                    if context_token._.shifter_type == 'neg' and negators_count == 0:\n",
    "                        apply_shifter(token, 'neg')\n",
    "                        negators_count += 1\n",
    "                        debug and print('Applying shifter ',context_token,' (',context_token._.shifter_type,') to token ',token,', new sentiment: ', token._.sentiment)\n",
    "    \n",
    "                    #adversatives\n",
    "                    if context_token._.shifter_type == 'adv':\n",
    "                        if context_token in foll_tokens:\n",
    "                            apply_shifter(token, 'adv_after')\n",
    "                            debug and print('Applying shifter ',context_token,' (',context_token._.shifter_type,') to token ',token,', new sentiment: ', token._.sentiment)\n",
    "                        elif context_token in pre_tokens:\n",
    "                            apply_shifter(token, 'adv_before')\n",
    "                            debug and print('Applying shifter ',context_token,' (',context_token._.shifter_type,') to token ',token,', new sentiment: ', token._.sentiment)\n",
    "                            \n",
    "                    #intensification  \n",
    "                    if context_token._.shifter_type == 'int':\n",
    "                        apply_shifter(token, 'int')\n",
    "                        debug and print('Applying shifter ',context_token,' (',context_token._.shifter_type,') to token ',token,', new sentiment: ', token._.sentiment)\n",
    "    \n",
    "                    #deintensification\n",
    "                    if context_token._.shifter_type == 'deint':\n",
    "                        apply_shifter(token, 'deint')\n",
    "                        debug and print('Applying shifter ',context_token,' (',context_token._.shifter_type,') to token ',token,', new sentiment: ', token._.sentiment)\n",
    "                    \n",
    "                token._.ignore_as_shifter = False\n",
    "        else:\n",
    "            token._.sentiment = 0\n",
    "            \n",
    "        \n",
    "\n",
    "    overall_sentiment = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_sentiment = 0\n",
    "        for token in sentence:\n",
    "            sentence_sentiment += token._.sentiment\n",
    "        #sentence_sentiment = sentence_sentiment / math.sqrt(len(sentence))\n",
    "        sentence._.sentiment = sentence_sentiment\n",
    "        overall_sentiment += sentence._.sentiment\n",
    "\n",
    "    if len(sentences)==0:\n",
    "        return 0\n",
    "    else:\n",
    "        overall_sentiment = overall_sentiment / len(sentences)\n",
    "        \n",
    "    return tokens, overall_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7639ab8d-d77e-46d0-ba8d-593ff5fd2056",
   "metadata": {},
   "source": [
    "### Dependency tree-based shifter pattern approach\n",
    "Returns overall sentiment from a list of tokens using dependency tree-based shifter pattern approach. First, it marks shifter words, assigns sentiment to tokens using the lexicon lookup, and identifies the head token of each sentence (1 sentence = 1 tree). Then, for each tree, it applies the shifter patterns, starting from the rightmost lowest node. Lastly, overall sentiment is calculated from all the sentences. The economics pattern (bigger is better / bigger is worse) is mixed in with the usual shifters (negation, intensification, deintensification, adversatives)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df5a9c6-040f-4ab4-ae44-9ab82297bef2",
   "metadata": {},
   "source": [
    "First, a helper method that allows traversing the dependency trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b6b44b9-561f-48ca-89c4-b0ba84246071",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generates token list out of tree, in order from bottom level to top, starting from the rightmost token on bottom level\n",
    "def bottom_to_top_traversal(root_token):\n",
    "    levels = defaultdict(list)\n",
    "    max_level = 0\n",
    "\n",
    "    def dfs(node, level):\n",
    "        nonlocal max_level\n",
    "        max_level = max(max_level, level)\n",
    "        levels[level].append(node)\n",
    "        for child in node.children:\n",
    "            dfs(child, level + 1)\n",
    "\n",
    "    dfs(root_token, 0)\n",
    "\n",
    "    for level in range(max_level, -1, -1):\n",
    "        for token in reversed(levels[level]):\n",
    "            yield token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d180862d-3f67-4e12-92b1-5f4a73c6f75c",
   "metadata": {},
   "source": [
    "The actual algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "12082708-0700-499a-943e-c8b95a31e5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_dep(tokens, lexicon, constants = constants, debug=False):\n",
    "\n",
    "    root_tokens = set()\n",
    "    \n",
    "    for token in tokens:\n",
    "\n",
    "        #some extensions have to be reset just in case\n",
    "        token._.sentiment = 0\n",
    "        token._.passed_sentiment = 0\n",
    "        token._.passed_shifters = []\n",
    "        \n",
    "     \n",
    "        if token.head == token:\n",
    "            root_tokens.add(token.head)\n",
    "\n",
    "        mark_as_shifter(token)\n",
    "        \n",
    "        token._.sentiment = get_token_sentiment(token, lexicon)\n",
    "\n",
    "    overall_sentiment = 0\n",
    "    \n",
    "    # for every dependency tree\n",
    "    for root_token in root_tokens:\n",
    "\n",
    "        negator_count = 0\n",
    "        \n",
    "        #iterate over tokens in tree from bottom to top, starting from the rightmost bottom token\n",
    "        for token in bottom_to_top_traversal(root_token):\n",
    "            \n",
    "\n",
    "            parent = token.head\n",
    "\n",
    "            #token isn't root token\n",
    "            if token != parent:\n",
    "\n",
    "                ignore_own_shifter = False\n",
    "\n",
    "                #assign polarity to economic term parent based on increase/decrease\n",
    "                if token.lemma_ in bigger_words:\n",
    "                    if parent.lemma_ in bigger_is_good_words:\n",
    "                        parent._.sentiment = 1\n",
    "                    if parent.lemma_ in bigger_is_bad_words:\n",
    "                        parent._.sentiment = -1\n",
    "                    ignore_own_shifter = True\n",
    "\n",
    "                if token.lemma_ in smaller_words:\n",
    "                    if parent.text in bigger_is_good_words:\n",
    "                        parent._.sentiment = -1\n",
    "                    if parent.text in bigger_is_bad_words:\n",
    "                        parent._.sentiment = 1\n",
    "                    ignore_own_shifter = True\n",
    "\n",
    "                #token is a shifter \n",
    "                if token._.shifter_type != 'none' and not ignore_own_shifter:\n",
    "\n",
    "                    # parent has sentiment - apply shifter\n",
    "                    if parent._.sentiment != 0: \n",
    "                        if not (token._.shifter_type == 'neg' and negator_count > 0): # only one negation allowed\n",
    "                            apply_shifter(parent, token._.shifter_type)\n",
    "                            debug and print('Applying shifter ',token,' (',token._.shifter_type,') to parent node ', parent, 'new sentiment: ', parent._.sentiment)\n",
    "                            if token._.shifter_type == 'neg':\n",
    "                                negator_count += 1\n",
    "                    #parent doesn't have sentiment - pass shifter to parent\n",
    "                    else: \n",
    "                        parent._.passed_shifters.append(token._.shifter_type)\n",
    "                        debug and print('Passing shifter ', token,' (',token._.shifter_type,') to parent node ', parent)\n",
    "\n",
    "                #token has been passed some shifters\n",
    "                if len(token._.passed_shifters) != 0:\n",
    "                    #if token has sentiment - apply passed shifter on itself\n",
    "                    if token._.sentiment != 0:\n",
    "                        for passed_shifter in token._.passed_shifters:\n",
    "                            apply_shifter(token, passed_shifter)\n",
    "                            debug and print('Applying inherited shifter (', passed_shifter,') to node',token, 'new sentiment: ', token._.sentiment)\n",
    "                            token._.passed_shifters.remove(passed_shifter)\n",
    "\n",
    "                \n",
    "                #finally, if token has any sentiment, it is passed to parent as passed sentiment\n",
    "                if(token._.sentiment != 0):\n",
    "                    parent._.passed_sentiment += token._.sentiment\n",
    "                    debug and print('Passing sentiment from child node ',token,' to parent node ', parent, '(',token._.sentiment,')')\n",
    "                \n",
    "                #if token has been passed sentiment, it also passes it to parent as passed sentiment\n",
    "                if(token._.passed_sentiment != 0):\n",
    "                    parent._.passed_sentiment += token._.passed_sentiment\n",
    "                    debug and print('Passing inherited sentiment from child node ',token ,' to parent node ', parent, '(',token._.passed_sentiment,')')\n",
    "\n",
    "            #token is root token\n",
    "            else:\n",
    "                #root token has no sentiment of its own (not a polarized word)\n",
    "                if token._.sentiment == 0:\n",
    "                    #if it has been passed any shifter, apply it to the passed sentiment it received\n",
    "                    for passed_shifter in token._.passed_shifters:\n",
    "                        if not (passed_shifter == 'neg' and negator_count>0): #only one negation allowed\n",
    "                            apply_shifter(token, passed_shifter, on_passed=True)\n",
    "                            debug and print('Applying inherited shifter (', passed_shifter,') to node', token, ' on inherited sentiment, new inherited sentiment: ', token._.passed_sentiment)\n",
    "\n",
    "                    #root token is a negator - most likely a negative verb, verbal negation negates whole sentence - negates any passed sentiment\n",
    "                    if token._.shifter_type=='neg' and negator_count==0:\n",
    "                        apply_shifter(token, 'neg', on_passed=True)\n",
    "                        debug and print('Applying sentential negation from root token ',token, ' on inherited sentiment, new inherited sentiment: ', token._.passed_sentiment)\n",
    "\n",
    "                #root token has sentiment on its own\n",
    "                else:\n",
    "                    #root token is negator - should negate any passed sentiment (sentential negation)\n",
    "                    if token._.shifter_type == 'neg' and token._.passed_sentiment != 0 and negator_count==0:\n",
    "                        apply_shifter(token, 'neg', on_passed=True)\n",
    "                        debug and print('Applying sentential negation from root token ',token, 'on inherited sentiment, new inherited sentiment: ', token._.passed_sentiment)\n",
    "\n",
    "                    #applies any passed shifters on itself and passed sentiment\n",
    "                    for passed_shifter in token._.passed_shifters:\n",
    "                       if not (passed_shifter == 'neg' and negator_count>0):\n",
    "                           apply_shifter(token, passed_shifter)\n",
    "                           print('Applying inherited shifter (', passed_shifter,') to node', token, 'new sentiment: ', token._.sentiment)\n",
    "                           if(token._.passed_sentiment != 0):\n",
    "                               apply_shifter(token, passed_shifter, on_passed = True)\n",
    "                               debug and print('Applying inherited shifter (', passed_shifter,') to node', token, ' on inherited sentiment, new inherited sentiment: ', token._.passed_sentiment)\n",
    "                        \n",
    "        overall_sentiment = overall_sentiment + root_token._.sentiment + root_token._.passed_sentiment\n",
    "\n",
    "    if len(root_tokens)==0:\n",
    "        return 0\n",
    "    else:\n",
    "        overall_sentiment = overall_sentiment / len(root_tokens)\n",
    "        \n",
    "    return tokens, overall_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992fc156-7a6e-49df-80c0-0c9cbcaa1721",
   "metadata": {},
   "source": [
    "### Applies sentiment analysis algorithm on a dataset\n",
    "\n",
    "Choose either proximity or dependency tree method using the parameter method ('proxi' or 'dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8fc27c83-e349-4f12-abc8-1a9cfc27770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_row_sentiment(row, lexicon, method):\n",
    "    tokens = row['tokens']\n",
    "    sentiment = 0\n",
    "    try:\n",
    "        if method in ['proxi', 'proximity']:\n",
    "            tokens, sentiment = get_sentiment_proxi(tokens, lexicon)\n",
    "        elif method in ['dep', 'dependency']:\n",
    "            tokens, sentiment = get_sentiment_dep(tokens, lexicon)\n",
    "    finally:       \n",
    "        return sentiment\n",
    "    \n",
    "def apply_sa_on_dataset(df, lexicon, method):\n",
    "    df = df.copy()\n",
    "    if method not in ['proxi', 'proximity', 'dep', 'dependency']:\n",
    "        print('No such method as '+method+'.')\n",
    "        return\n",
    "    df['sentiment'] = df.apply(lambda row: get_df_row_sentiment(row, lexicon, method), axis=1)\n",
    "    df['sentiment_pred'] = df['sentiment'].apply(lambda x: 'p' if x > 0 else ('n' if x < 0 else '0'))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d7cbff-ac64-419b-94d4-60caeaae4820",
   "metadata": {},
   "source": [
    "### Load lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cfdb24ea-78f9-4a0a-9187-12d1ce105ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lexicon loaded from text_resources/lexicons/sublex_1_0.csv\n",
      "lexicon loaded from text_resources/lexicons/affincz.txt\n"
     ]
    }
   ],
   "source": [
    "sublex = load_czech_sublex_dict('text_resources/lexicons/sublex_1_0.csv')\n",
    "affin = load_affincz('text_resources/lexicons/affincz.txt')\n",
    "#autolex = generate_lexicon_dict(preprocessed_df, min_occurence=2, min_abs_value=0.5, simple_sentiment=True)\n",
    "#joinedlex = dict(autolex)\n",
    "#joinedlex = dict(affin)\n",
    "#joinedlex.update(affin)\n",
    "#joinedlex.update(sublex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c6fd75-62e0-4f01-bd1a-452a9b65c9d6",
   "metadata": {},
   "source": [
    "### Lemmatization test\n",
    "For testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ecfe6c5f-715f-4b79-9d17-b7ef7419513f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: není\n",
      "Lemma: být\n",
      "POS: AUX\n",
      "Mood=Ind|Number=Sing|Person=3|Polarity=Neg|Tense=Pres|VerbForm=Fin|Voice=Act\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('není')\n",
    "for token in doc:\n",
    "    print('Input: '+token.text)\n",
    "    print('Lemma: '+token.lemma_)\n",
    "    print('POS: '+token.pos_)\n",
    "    print(token.morph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6913509-9e3a-4727-b79c-5c521bf3edce",
   "metadata": {},
   "source": [
    "### Sentiment analysis for one string\n",
    "#### Proximity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8650ab2f-2743-4aaa-bc7d-da1beab13847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kytara  --- sentiment:  0\n",
      "nezněla  --- sentiment:  0 , shifter type:  neg\n",
      "vskutku  --- sentiment:  0 , shifter type:  int\n",
      "špatně  --- sentiment:  -1.0\n",
      "----------------------------\n",
      "Evaluating context cluster of token  špatně\n",
      "Applying shifter  nezněla  ( neg ) to token  špatně , new sentiment:  0.5\n",
      "Applying shifter  vskutku  ( int ) to token  špatně , new sentiment:  0.9\n",
      "Overall sentiment: 0.9\n"
     ]
    }
   ],
   "source": [
    "doc = preprocess_text('Kytara nezněla vskutku špatně')\n",
    "tokens = get_tokens(doc)\n",
    "for token in tokens:\n",
    "    mark_as_shifter(token)\n",
    "    if(token._.shifter_type != 'none'):\n",
    "       print(token, ' --- sentiment: ', get_token_sentiment(token, sublex), ', shifter type: ', token._.shifter_type) \n",
    "    else:\n",
    "        print(token, ' --- sentiment: ',get_token_sentiment(token, sublex))\n",
    "print('----------------------------')       \n",
    "tokens, overall_sentiment = get_sentiment_proxi(tokens, sublex, debug=True)\n",
    "\n",
    "print('Overall sentiment: ' + str(overall_sentiment))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4e8f62-3822-4eb4-884a-dc819f42f7b8",
   "metadata": {},
   "source": [
    "#### Dependency tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4b990536-d1ce-47c1-99c5-802e2c54ab27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kytara  --- sentiment:  0\n",
      "nezněla  --- sentiment:  0 , shifter type:  neg\n",
      "vskutku  --- sentiment:  0 , shifter type:  int\n",
      "špatně  --- sentiment:  -1.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"cs\" id=\"f0a6b300d02c4343bf6074c94d2f383e-0\" class=\"displacy\" width=\"410\" height=\"227.0\" direction=\"ltr\" style=\"max-width: none; height: 227.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Kytara</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"140\">nezněla</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"140\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"230\">vskutku</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"230\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"320\">špatně</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"320\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f0a6b300d02c4343bf6074c94d2f383e-0-0\" stroke-width=\"2px\" d=\"M70,92.0 C70,47.0 135.0,47.0 135.0,92.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f0a6b300d02c4343bf6074c94d2f383e-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,94.0 L62,82.0 78,82.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f0a6b300d02c4343bf6074c94d2f383e-0-1\" stroke-width=\"2px\" d=\"M160,92.0 C160,47.0 225.0,47.0 225.0,92.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f0a6b300d02c4343bf6074c94d2f383e-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M225.0,94.0 L233.0,82.0 217.0,82.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f0a6b300d02c4343bf6074c94d2f383e-0-2\" stroke-width=\"2px\" d=\"M160,92.0 C160,2.0 320.0,2.0 320.0,92.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f0a6b300d02c4343bf6074c94d2f383e-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M320.0,94.0 L328.0,82.0 312.0,82.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passing sentiment from child node  špatně  to parent node  nezněla ( -1.0 )\n",
      "Passing shifter  vskutku  ( int ) to parent node  nezněla\n",
      "Applying inherited shifter ( int ) to node nezněla  on inherited sentiment, new inherited sentiment:  -1.8\n",
      "Applying sentential negation from root token  nezněla  on inherited sentiment, new inherited sentiment:  0.9\n",
      "Overall sentiment: 0.9\n"
     ]
    }
   ],
   "source": [
    "doc = preprocess_text('Kytara nezněla vskutku špatně')\n",
    "tokens = get_tokens(doc)\n",
    "for token in tokens:\n",
    "    mark_as_shifter(token)\n",
    "    if(token._.shifter_type != 'none'):\n",
    "       print(token, ' --- sentiment: ', get_token_sentiment(token, sublex), ', shifter type: ', token._.shifter_type) \n",
    "    else:\n",
    "        print(token, ' --- sentiment: ',get_token_sentiment(token, sublex))          \n",
    "displacy.render(doc, style=\"dep\", jupyter=True, options={\"distance\": 90})\n",
    "tokens = get_tokens(doc)\n",
    "tokens, overall_sentiment = get_sentiment_dep(tokens, sublex, debug=True)\n",
    "print('Overall sentiment: ' + str(overall_sentiment))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4deffe46-92d8-44d8-8752-306796a7d53b",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Make sure to run all previous cells before running evaluations. Contains evaluations for Facebook, CSFD, Exctracted and Synthetic datasets. Parameters can be changed. Which lexicons will be used can be changed. Method (proximity, dependency) can be changed. All evaluations are done using 10-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61bcd00-d0dc-4401-81e8-58b52f86c981",
   "metadata": {},
   "source": [
    "### Run this first - helper for generating and averaging classification reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d4cd3ee9-1e14-44cb-b0cd-15fa0acc3514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_report(label_col, pred_col, output_dict=False):\n",
    "    report = classification_report(label_col, pred_col, output_dict=output_dict)\n",
    "    return report\n",
    "\n",
    "def avg_reports(*args):\n",
    "    mean_dict = dict()\n",
    "    for label in reports[0].keys():\n",
    "        dictionary = dict()\n",
    "\n",
    "        if label in 'accuracy':\n",
    "            mean_dict[label] = sum(d[label] for d in reports) / len(reports)\n",
    "            continue\n",
    "\n",
    "        for key in reports[0][label].keys():\n",
    "            dictionary[key] = sum(d[label][key] for d in reports) / len(reports)\n",
    "        mean_dict[label] = dictionary\n",
    "\n",
    "    return mean_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35087db6-1c0f-4494-a201-7a448c9ead98",
   "metadata": {},
   "source": [
    "### Facebook data evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aacf3802-a48a-404e-8d15-cf7b1e3d7e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df = load_from_pickle('datasets/facebook.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2348e7ca-b9c3-4aa8-ad3e-1645eca8a898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lexicon loaded from text_resources/lexicons/sublex_1_0.csv\n",
      "lexicon loaded from text_resources/lexicons/affincz.txt\n",
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "{'0': {'precision': 0.6328396220490873, 'recall': 0.49274075299265796, 'f1-score': 0.5538765274586636, 'support': 517.4}, 'n': {'precision': 0.43213861263871245, 'recall': 0.2612506581280924, 'f1-score': 0.3250802781010592, 'support': 199.1}, 'p': {'precision': 0.3894562069273318, 'recall': 0.679723215579535, 'f1-score': 0.49480407710802093, 'support': 258.7}, 'accuracy': 0.494978352248844, 'macro avg': {'precision': 0.4848114805383772, 'recall': 0.4779048755667617, 'f1-score': 0.4579202942225812, 'support': 975.2}, 'weighted avg': {'precision': 0.5280609198666633, 'recall': 0.494978352248844, 'f1-score': 0.4918160461976823, 'support': 975.2}}\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "sublex = load_czech_sublex_dict('text_resources/lexicons/sublex_1_0.csv')\n",
    "affin = load_affincz('text_resources/lexicons/affincz.txt')\n",
    "\n",
    "reports = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(kfold.split(preprocessed_df)):\n",
    "    train = preprocessed_df.iloc[train_idx]\n",
    "    test = preprocessed_df.iloc[test_idx]\n",
    "    autolex = generate_lexicon_dict(train, min_occurence=5, min_abs_value=0.4, simple_sentiment=True)\n",
    "    #joinedlex = sublex.copy()\n",
    "    #joinedlex.update(autolex)\n",
    "    result_df = apply_sa_on_dataset(test, sublex, method='proxi')\n",
    "    report = get_report(result_df['labels'], result_df['sentiment_pred'], output_dict=True)\n",
    "    reports.append(report)\n",
    "\n",
    "avg_report = avg_reports(reports)\n",
    "print(avg_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a42baf-79b6-4eca-b796-caf65d1b9b71",
   "metadata": {},
   "source": [
    "### CSFD data evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a404c09d-0f62-4f4e-9f8d-d845f827f21b",
   "metadata": {},
   "source": [
    "Make sure that CSFD data was preprocessed and serialized first if you need to see the CSFD evaluation (it is disabled by default due to it taking a long time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7c7b93fe-9c13-4c93-a816-36c380694fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df = load_from_pickle('datasets/csfd.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "de2ffcfc-c82c-4be3-96ca-648838300c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lexicon loaded from text_resources/lexicons/sublex_1_0.csv\n",
      "lexicon loaded from text_resources/lexicons/affincz.txt\n",
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "{'0': {'precision': 0.45847198705800973, 'recall': 0.4794422275704098, 'f1-score': 0.4686844422931947, 'support': 3076.8}, 'n': {'precision': 0.694541192022708, 'recall': 0.46688852605234155, 'f1-score': 0.5583646936653408, 'support': 2971.6}, 'p': {'precision': 0.6202215551295744, 'recall': 0.7874472039502504, 'f1-score': 0.6938679829994883, 'support': 3089.7}, 'accuracy': 0.5794858754858064, 'macro avg': {'precision': 0.5910782447367641, 'recall': 0.5779259858576673, 'f1-score': 0.5736390396526747, 'support': 9138.1}, 'weighted avg': {'precision': 0.5899625723421587, 'recall': 0.5794858754858064, 'f1-score': 0.5739945678673488, 'support': 9138.1}}\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "sublex = load_czech_sublex_dict('text_resources/lexicons/sublex_1_0.csv')\n",
    "affin = load_affincz('text_resources/lexicons/affincz.txt')\n",
    "\n",
    "reports = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(kfold.split(preprocessed_df)):\n",
    "    train = preprocessed_df.iloc[train_idx]\n",
    "    test = preprocessed_df.iloc[test_idx]\n",
    "    autolex = generate_lexicon_dict(train, min_occurence=25, min_abs_value=0.5, simple_sentiment=True)\n",
    "    #joinedlex = sublex.copy()\n",
    "    #joinedlex.update(autolex)\n",
    "    result_df = apply_sa_on_dataset(test, autolex, method='proxi')\n",
    "    report = get_report(result_df['labels'], result_df['sentiment_pred'], output_dict=True)\n",
    "    reports.append(report)\n",
    "\n",
    "avg_report = avg_reports(reports)\n",
    "print(avg_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448f8d9a-3738-41b5-a7f2-f9db3c7a54fe",
   "metadata": {},
   "source": [
    "### Extracted dataset evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6d44588f-015a-4dff-b65a-ee15f2696449",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df = load_from_pickle('datasets/extracted.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d37a2b68-e663-44bc-a6e2-e757ffc6de83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "{'0': {'precision': 0.4449063233699797, 'recall': 0.6304585679717336, 'f1-score': 0.5212716731118343, 'support': 193.2}, 'n': {'precision': 0.6419769104301432, 'recall': 0.49466944257089207, 'f1-score': 0.5572701766373085, 'support': 193.4}, 'p': {'precision': 0.6081213209470266, 'recall': 0.49428656139308363, 'f1-score': 0.5449014097522296, 'support': 193.5}, 'accuracy': 0.5392186479909787, 'macro avg': {'precision': 0.5650015182490498, 'recall': 0.5398048573119031, 'f1-score': 0.5411477531671242, 'support': 580.1}, 'weighted avg': {'precision': 0.5662468382251763, 'recall': 0.5392186479909787, 'f1-score': 0.5413525180496302, 'support': 580.1}}\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "#sublex = load_czech_sublex_dict('text_resources/lexicons/sublex_1_0.csv')\n",
    "#affin = load_affincz('text_resources/lexicons/affincz.txt')\n",
    "\n",
    "reports = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(kfold.split(preprocessed_df)):\n",
    "    train = preprocessed_df.iloc[train_idx]\n",
    "    test = preprocessed_df.iloc[test_idx]\n",
    "    autolex = generate_lexicon_dict(train, min_occurence=5, min_abs_value=0.5, simple_sentiment=False)\n",
    "    #joinedlex = sublex.copy()\n",
    "    #joinedlex.update(autolex)\n",
    "    result_df = apply_sa_on_dataset(test, autolex, method='proxi')\n",
    "    report = get_report(result_df['labels'], result_df['sentiment_pred'], output_dict=True)\n",
    "    reports.append(report)\n",
    "\n",
    "avg_report = avg_reports(reports)\n",
    "print(avg_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c361dc-5455-4a5a-846d-98fe7dcb69d2",
   "metadata": {},
   "source": [
    "### Synthetic dataset evauluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b77a4e00-0ee6-4be5-87cf-9413f29ba7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df = load_from_pickle('datasets/synthetic.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "85acf426-b212-4fe8-94d3-c54dfeef3185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lexicon loaded from text_resources/lexicons/sublex_1_0.csv\n",
      "lexicon loaded from text_resources/lexicons/affincz.txt\n",
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "{'0': {'precision': 0.819811144265362, 'recall': 0.5396400342738978, 'f1-score': 0.6506030899219202, 'support': 250.0}, 'n': {'precision': 0.8130077966234891, 'recall': 0.8980815589428023, 'f1-score': 0.8529720947664691, 'support': 250.0}, 'p': {'precision': 0.7257059987594261, 'recall': 0.8983096962820077, 'f1-score': 0.8026649736697713, 'support': 250.0}, 'accuracy': 0.7788, 'macro avg': {'precision': 0.7861749798827591, 'recall': 0.7786770964995694, 'f1-score': 0.7687467194527202, 'support': 750.0}, 'weighted avg': {'precision': 0.7872536783256565, 'recall': 0.7788, 'f1-score': 0.7693394649848649, 'support': 750.0}}\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "sublex = load_czech_sublex_dict('text_resources/lexicons/sublex_1_0.csv')\n",
    "affin = load_affincz('text_resources/lexicons/affincz.txt')\n",
    "\n",
    "reports = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(kfold.split(preprocessed_df)):\n",
    "    train = preprocessed_df.iloc[train_idx]\n",
    "    test = preprocessed_df.iloc[test_idx]\n",
    "    autolex = generate_lexicon_dict(train, min_occurence=10, min_abs_value=0.5, simple_sentiment=True)\n",
    "    #joinedlex = sublex.copy()\n",
    "    #joinedlex.update(autolex)\n",
    "    result_df = apply_sa_on_dataset(test, autolex, method='proxi')\n",
    "    report = get_report(result_df['labels'], result_df['sentiment_pred'], output_dict=True)\n",
    "    reports.append(report)\n",
    "    \n",
    "avg_report = avg_reports(reports)\n",
    "print(avg_report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529f59db-ae02-4891-88bc-64c0004f4b3b",
   "metadata": {},
   "source": [
    "## Examples for automatic lexicon generation\n",
    "\n",
    "Parameters can be changed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6d5f4a-159d-47b4-8edf-936a5b164b57",
   "metadata": {},
   "source": [
    "### Example - generate auto lexicon from fb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "55645242-5d7a-44a5-bc23-82c8c2a1d3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "         lemma   n    p   0  total   n_ratio   p_ratio   0_ratio  \\\n",
      "0        drahý  26    1  10     37  0.702703  0.027027  0.270270   \n",
      "1         vůně   8  100  15    123  0.065041  0.813008  0.121951   \n",
      "2       chápat  26    0  19     45  0.577778  0.000000  0.422222   \n",
      "3        pěkný   9   67  15     91  0.098901  0.736264  0.164835   \n",
      "4        super  12   95  10    117  0.102564  0.811966  0.085470   \n",
      "5      dovolat  19    2   8     29  0.655172  0.068966  0.275862   \n",
      "6       skvělý   4   60   6     70  0.057143  0.857143  0.085714   \n",
      "7     oblíbený   1   27   2     30  0.033333  0.900000  0.066667   \n",
      "8      smlouva  27    1  21     49  0.551020  0.020408  0.428571   \n",
      "9     zákazník  43    2  18     63  0.682540  0.031746  0.285714   \n",
      "10         síť  21    1  18     40  0.525000  0.025000  0.450000   \n",
      "11   odpovědět  17    1  13     31  0.548387  0.032258  0.419355   \n",
      "12    procento  18    0  12     30  0.600000  0.000000  0.400000   \n",
      "13      signál  20    2  11     33  0.606061  0.060606  0.333333   \n",
      "14     výborný   2   28   3     33  0.060606  0.848485  0.090909   \n",
      "15      krásný   6  177  15    198  0.030303  0.893939  0.075758   \n",
      "16        přát   4   66  18     88  0.045455  0.750000  0.204545   \n",
      "17      úžasný   2   53   5     60  0.033333  0.883333  0.083333   \n",
      "18  gratulovat   3   37   5     45  0.066667  0.822222  0.111111   \n",
      "19       vonět   3   18   9     30  0.100000  0.600000  0.300000   \n",
      "20       Super   2   27   2     31  0.064516  0.870968  0.064516   \n",
      "21       zvíře   2   16   7     25  0.080000  0.640000  0.280000   \n",
      "22    nádherný   1   72   2     75  0.013333  0.960000  0.026667   \n",
      "23       těšit   0   61  14     75  0.000000  0.813333  0.186667   \n",
      "24     milovat   0   42   8     50  0.000000  0.840000  0.160000   \n",
      "\n",
      "    sentiment_value  \n",
      "0         -0.675676  \n",
      "1          0.747967  \n",
      "2         -0.577778  \n",
      "3          0.637363  \n",
      "4          0.709402  \n",
      "5         -0.586207  \n",
      "6          0.800000  \n",
      "7          0.866667  \n",
      "8         -0.530612  \n",
      "9         -0.650794  \n",
      "10        -0.500000  \n",
      "11        -0.516129  \n",
      "12        -0.600000  \n",
      "13        -0.545455  \n",
      "14         0.787879  \n",
      "15         0.863636  \n",
      "16         0.704545  \n",
      "17         0.850000  \n",
      "18         0.755556  \n",
      "19         0.500000  \n",
      "20         0.806452  \n",
      "21         0.560000  \n",
      "22         0.946667  \n",
      "23         0.813333  \n",
      "24         0.840000  \n"
     ]
    }
   ],
   "source": [
    "preprocessed_df = load_from_pickle('datasets/facebook.pickle')\n",
    "lexicon = generate_lexicon(preprocessed_df, min_occurence=25, min_abs_value = 0.5, additional_stats=True)\n",
    "print(lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a7c971-7b88-4c92-b5e8-9df19d0bbd0a",
   "metadata": {},
   "source": [
    "### Example - generate auto lexicon from csfd data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e78682e5-232d-4ed1-8a18-2508b960f449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "            lemma     p     n     0  total   p_ratio   n_ratio   0_ratio  \\\n",
      "0          příběh  4329  1228  3178   8735  0.495592  0.140584  0.363824   \n",
      "1      potvrzovat    88    37    42    167  0.526946  0.221557  0.251497   \n",
      "2        pravidlo   163    58    84    305  0.534426  0.190164  0.275410   \n",
      "3       perfektný    33     1     5     39  0.846154  0.025641  0.128205   \n",
      "4          muzika   166    38    84    288  0.576389  0.131944  0.291667   \n",
      "...           ...   ...   ...   ...    ...       ...       ...       ...   \n",
      "13573     emzácký     0     3     4      7  0.000000  0.428571  0.571429   \n",
      "13574   kostýmový     0     2     3      5  0.000000  0.400000  0.600000   \n",
      "13575       usrat     0     5     0      5  0.000000  1.000000  0.000000   \n",
      "13576     hasnout     0     2     3      5  0.000000  0.400000  0.600000   \n",
      "13577   pochůzkář     0     3     3      6  0.000000  0.500000  0.500000   \n",
      "\n",
      "       sentiment_value  \n",
      "0             0.355009  \n",
      "1             0.305389  \n",
      "2             0.344262  \n",
      "3             0.820513  \n",
      "4             0.444444  \n",
      "...                ...  \n",
      "13573        -0.428571  \n",
      "13574        -0.400000  \n",
      "13575        -1.000000  \n",
      "13576        -0.400000  \n",
      "13577        -0.500000  \n",
      "\n",
      "[13578 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_df = load_from_pickle('datasets/csfd.pickle')\n",
    "lexicon = generate_lexicon(preprocessed_df, min_occurence=5, additional_stats=True)\n",
    "print(lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e8b21d-8d82-426e-b4ff-8023880d9206",
   "metadata": {},
   "source": [
    "### Example - generate auto lexicon from extracted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "98f60bdd-d087-4820-a1c4-da96f0604f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "          lemma   p  n  0  total   p_ratio   n_ratio   0_ratio  \\\n",
      "0     vyžadovat   1  5  6     12  0.083333  0.416667  0.500000   \n",
      "1       pomáhat  17  1  3     21  0.809524  0.047619  0.142857   \n",
      "2    efektivita   7  0  0      7  1.000000  0.000000  0.000000   \n",
      "3     umožňovat  11  1  3     15  0.733333  0.066667  0.200000   \n",
      "4       nákupní   7  1  5     13  0.538462  0.076923  0.384615   \n",
      "..          ...  .. .. ..    ...       ...       ...       ...   \n",
      "897   pracující   0  3  3      6  0.000000  0.500000  0.500000   \n",
      "898      záloha   0  5  3      8  0.000000  0.625000  0.375000   \n",
      "899    narozený   0  3  2      5  0.000000  0.600000  0.400000   \n",
      "900    náhradní   0  3  2      5  0.000000  0.600000  0.400000   \n",
      "901     pasažér   0  2  3      5  0.000000  0.400000  0.600000   \n",
      "\n",
      "     sentiment_value  \n",
      "0          -0.333333  \n",
      "1           0.761905  \n",
      "2           1.000000  \n",
      "3           0.666667  \n",
      "4           0.461538  \n",
      "..               ...  \n",
      "897        -0.500000  \n",
      "898        -0.625000  \n",
      "899        -0.600000  \n",
      "900        -0.600000  \n",
      "901        -0.400000  \n",
      "\n",
      "[902 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_df = load_from_pickle('datasets/extracted.pickle')\n",
    "lexicon = generate_lexicon(preprocessed_df, min_occurence=5, additional_stats=True)\n",
    "print(lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7bb8fd-37eb-42a4-bc6f-709faa235508",
   "metadata": {},
   "source": [
    "### Example - generate auto lexicon from synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "facaa56d-1b37-408e-8864-a96ef932fd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting lexicon generation\n",
      "finished lexicon generation\n",
      "         lemma     p    n   0  total   p_ratio   n_ratio   0_ratio  \\\n",
      "0      balíček    65    3   8     76  0.855263  0.039474  0.105263   \n",
      "1    povzbudit    17    0   0     17  1.000000  0.000000  0.000000   \n",
      "2         růst  1392  413  44   1849  0.752839  0.223364  0.023797   \n",
      "3         malý   300   40  44    384  0.781250  0.104167  0.114583   \n",
      "4      střední   274   25  54    353  0.776204  0.070822  0.152975   \n",
      "..         ...   ...  ...  ..    ...       ...       ...       ...   \n",
      "528    působit     0    3   5      8  0.000000  0.375000  0.625000   \n",
      "529    uzavřít     0    3   2      5  0.000000  0.600000  0.400000   \n",
      "530     uvádět     0    2   4      6  0.000000  0.333333  0.666667   \n",
      "531    západní     0    2   4      6  0.000000  0.333333  0.666667   \n",
      "532    rozruch     0    2   4      6  0.000000  0.333333  0.666667   \n",
      "\n",
      "     sentiment_value  \n",
      "0           0.815789  \n",
      "1           1.000000  \n",
      "2           0.529475  \n",
      "3           0.677083  \n",
      "4           0.705382  \n",
      "..               ...  \n",
      "528        -0.375000  \n",
      "529        -0.600000  \n",
      "530        -0.333333  \n",
      "531        -0.333333  \n",
      "532        -0.333333  \n",
      "\n",
      "[533 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_df = load_from_pickle('datasets/synthetic.pickle')\n",
    "lexicon = generate_lexicon(preprocessed_df, min_occurence=5, additional_stats=True)\n",
    "print(lexicon)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
